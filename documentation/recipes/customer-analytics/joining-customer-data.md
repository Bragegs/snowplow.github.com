---
layout: page
group: documentation
sub_group: customer
title: Joining Snowplow data to other customer data sets
shortened-link: Joining Snowplow data to other customer data sets
weight: 3
permalink: /documentation/recipes/customer-analytics/joining-customer-data.html
redirect_from:
  - /analytics/recipes/customer-analytics/joining-customer-data.html
  - /analytics/customer-analytics/joining-customer-data.html
---

# Joining Snowplow engagement data with other sources of customer data into customer datamarts

The value of data sets increases when they are joined with other, complimentary data sets. No where is this more true than in the case of customer data. Busiensses have for years spend huge sums of money integrating multiple customer data sets (incl. CRM data, financial data, offline sales data, email marketing data etc.) into datawarehouses, datamarts and single customer views to provide a complete picture of their userbase. Until Snowplow, however, integrating web analytics with other customer data sources has always been difficult if not impossible.

In this section, we walk through the process of integrating Snowplow data with other data sources:

1. [Why integrate web analytics data with other customer data sources](#why)
2. [Necessary ingredients for successful integration](#ingredients)
3. [Uploading customer data into Snowplow](#upload)
4. [Joining customer data with Snowplow to create an integrated customer datamart](#join)

<div class="html">
<a name="why"><h2>1. Why integrate web analytics data with other customer data sources?</h2></a>
</div>

To date many companies have become very good at using CRM, loyalty and sales data to understand, segment and serve their customer base. Typically, however, they have **not** used web analytics data to understand their customers, even though how users engage with brands and companies through their website and applications provides some of the most revealing data about who a person is, there motivation for engaging with a brand / product / service and how happy they are with that brand / product and service. To illustrate how revealing web analytics data can be, consider:

* Data from online retail sites can reveal what products a user is interested in purchasing, before they potentially visit a store
* *When* a user visits can often tell us a lot about a user: do they visit a website during office hours, as a form of distraction? Or at home? Does their browsing behavior change based on day / time?
* Purchase data can also reveal lifestyle interests: for example, someone shoppingi for nappies is most likely to have a young family
* Data from a content or media site can reveal what specific interests a user has
* If a user repeatedly visits a particular product page, but does not buy, it suggests that something is going wrong with the purchase flow, and the user is likely to become frustrated with the service provider. (And potentially stop being a customer)
* Web analytics data might reveal how price sensitive a user is i.e. does their propensity to buy increase dramatically during promotions?
* Web analytics data can be used to distinguish focused buyers (who come on a website looking for a specific product, and zoom in on it quickly) from those who are interested in browsing and then end up buying

By integrating Snowplow data with other customer data, it is possible learn additional things about your customers, and use that to drive your marketing and platform development decisions.

Potential sources of customer data to integrate with:

* CRM incl. loyalty (e.g. generated by point-of-sale systems, or loyalty cards)
* Marketing databases (e.g. email, direct mail, ad server)
* Social media (e.g. Facebook, Twitter)

<div class="html">
<a name="ingredients"><h2>2. Necessary ingredients for successful integration</h2></a>
</div>

Integrating any two data sources requires that:

* Both sources can be queried via a single interface. (In practice, this often means *copying* one or both source so that all the data exists in a single database)
* Ability able to join lines of data based on a customer identifier

Given the two above requirements, it becomes clear why web analytics data has not been integrated with other customer data sources:

* Web analytics data volumes are typically enormous, making *copying* the data hard. (Snowplow *fixes* this problem by enabling you to import your data into Snowplow, rather than export your web analytics data out of it.)
* Web analytics programs are typically bad at reliably identifying users. (Snowplow *fixes* this problem by exposing a `user_id`, `domain_userid` and `network_userid` and making it striaghtforward to map those user identifiers with identifiers from other systems.)

<div class="html">
<a name="upload"><h2>3. Uploading data into Snowplow</h2></a>
</div>

In order two join two data sets, you need to be able to run a query across both of them. Typically, this means getting copies of them in the same system. There are three options:

1. Export data out of Snowplow and into the system that the data you want to integrate it with lives e.g. your CRM system
2. Import the data you want to integrate with Snowplow out the system it usually lives (e.g. your CRM system) and import it into Snowplow
3. Export data out of Snowplow and out of the system you want to integrate it with, and import both data sets into a third system to run the join

Whilst all three options are possible, more often then not we recommend companies use option 2 i.e. import the customer data they want to join with Snowplow into Snowplow. (Or more specifically, if they are querying Snowplow data in Hive, import the data into Hive. If they are querying Snowplow data in Redshift, import the data into Redshift.) The reason is relatively straightforward: Snowplow data sets tend to be very large: that is why we use either Hive or Redshift to store and query the data, as both these platforms are built for scale. Other sources of customer data e.g. CRM data are typically much smaller scale, so moving them into Hive / Redshift is usually more straightforward then moving millions of lines of Snowplow data into a non-Hive / non-Redshift data store.

1. [Importing your data into Redshift] (#redshift-import)
2. [Importing your data into S3 / Hive](#hive-import)

<div class="html">
<a name="redshift-import"><h3>3.1 Importing your data into Redshift</h3></a>
</div>

If you are using Redshift to store and query your Snowplow data, uploading other sources of customer data into your Redshift cluster is reasonably straightforward.

Because uploading data into Redshift is fastest from S3, we generally recommend saving the data in tab delimited format (being careful to remove any tabs from the data), and then uploading those text files to S3. Bulk loading the data from S3 into Redshift is then straightforward. (For details on how to bulk load data from S3 into Redshift - see [here][bulk-load-data-from-s3-to-redshift].) Note you will need to create new tables in Redshift with a schema that match the format of the exported data in S3, prior to performing the bulk upload.

To maintain a regular update of data from your other customer data sources into Redshift for analysing alongside Snowplow, it makes sense either to script an ETL process, or to use an ETL tool. Currently there are not any Redshift compatible tools ETL tools that we have tested. However, we expect many ETL tools to roll out Redshift support over the next couple of months. In addition, the Snowplow team can setup those regular ETL steps as part of our [professional services][pro-services] offering.

<div class="html">
<a name="hive-import"><h3>3.2 Importing data into Hive</h3></a>
</div>

If you are running Snowplow using Hive on Amazon's S3 / EMR infrastructure, uploading data into Hive is a three step process:

1. Export the data from your system in a format suitable for Hive. (In practice, this generally means some kind of text-delimited format e.g. tab-delimited or comma-delimited)
2. Upload the data into S3
3. Create an external table in Hive that references the data, with the correct field definitions

#### 3.2.1 Exporting your data in a format suitable for Hive

How you export customer data out of your systems will vary depending on the system. In general, however, nearly all cutomer-data applications will enable you to export customer records as CSV files.

Some useful things to note about the format of the exported CSV file. (If the CSV file is not in the appropriate format, it will need to be transformed before being uploaded into S3):

1. It is important you know the structure of the CSV file, including the column names and data types. This information will be used when you define a new table in Hive that points at this data
2. At least one of the columns should be a user identifier that will be used to do the join
3. If possible, the CSV should not contain the column titles themselves, as this will make importing them into Hive more straightforward

#### 3.2.2 Uploading data into S3

Uploading data into S3 is reasonably straight forward: this can be done via the [Amazon Web Services UI][aws-console] or an S3 interface tool like [Bucket Explorer][bucket-explorer].

#### 3.2.3 Defining your data in Hive

Once your data is on S3, you need to tell Hive about it, so you can query it. To do that, you'll need to define a Hive table for it:

{% highlight mysql %}
/* HiveQL / MySQL / Redshift pseudo-code */
CREATE EXTERNAL TABLE {{table_name}}
(
	field_1 data_type,
	field_2 data_type,
	...
	field_n data_type
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
LOCATION 's3://LOCATION-BUCKET/LOCATION-PATH/';
{% endhighlight %}

To check that your data has been successfully uploaded, you can run some sample queries within Hive e.g. to view the first 10 rows:

{% highlight mysql %}
/* HiveQL / MySQL */
SELECT * FROM {{table_name}} LIMIT 10;
{% endhighlight %}

<div class="html">
<a name="join"><h2>4. Joining 3rd party customer data with Snowplow data</h2></a>
</div>

Now that all your customer data sets are available in Redshift or Hive, you are in a position to run an analysis across all customer data sets.</a>

To do so, however, you need to map users between the different systems: mapping users identified in Snowplow by the `domain_userid`, `network_userid` and `user_id`, with customers as identified by a `customer_id` (or something similar) in your other customer data sets. This requires firing a Snowplow event with the `customer_id` at some stage in the customer journey when the ID is available, so that it can be assigned to the Snowplow `user_id`. The most common way of achieving this is to fire the event at login events, as described in the [identifying users with Snowplow][identifying-users] section of this Cookbook. Assuming this has been done, you can generate the mapping by executing the following SQL query:

{% highlight mysql %}
/* HiveQL / MySQL / Redshift */
SELECT
user_id,
domain_userid,
network_userid
FROM "atomic".events
GROUP BY user_id, domain_userid, network_userid;
{% endhighlight %}

You can then perform queries that join both data tables, the Snowplow `events` table and your 2nd table of imported customer data via the mapping table above:

{% highlight mysql %}
/* HiveQL / MySQL / Redshift pseudo-code */
SELECT
e.field_1,
e.field_2,
...
c.field_1,
c.field_2
FROM external_customer_data c
JOIN (
	SELECT
	user_id,
	domain_userid,
	network_userid
	FROM "atomic".events
	GROUP BY user_id, domain_userid, network_userid
) m
ON c.user_id = m.user_id
JOIN "atomic".events
ON e.domain_userid = m.domain_userid;
{% endhighlight %}

## Happy integrating Snowplow data with other sources of customer data?

Find out [how to calculate customer lifetime value][clv] using Snowplow

[aws-console]: http://console.aws.amazon.com
[bucket-explorer]: http://www.bucketexplorer.com/
[navicat]: http://www.navicat.com/
[jitterbit]: http://www.jitterbit.com/
[identifying-users]: identifying-users.html#login-events
[clv]: customer-lifetime-value.html
[bulk-load-data-from-s3-to-redshift]: http://docs.aws.amazon.com/redshift/latest/dg/t_Loading-data-from-S3.html
[pro-services]: /services/index.html
