---
layout: post
shortenedlink: Deduplicating the event ID in Redshift
title: Deduplicating the event ID in Redshift
tags: [analytics, data modeling, events]
author: Christophe
category: analytics
---

The Snowplow pipeline outputs a data stream in which each line represents a single event. Each event comes with an identifier, the event ID, which was generated by the tracker and is – or rather should be – unique. However, after having used Snowplow for a while, users often notice that some events share an event ID. These duplicates are sometimes introduced within the Snowplow pipeline, but more often a consequence of a client-side environment that produces duplicate events.

This blogposts covers:

1. [Is event ID unique?](/blog/2015/07/24/)
2. [Do duplicated events cause problems?](/blog/2015/07/24/)
3. [What can cause duplicates?](/blog/2015/07/24/)
4. [Implementing and upgrading SQL data models](/blog/2015/07/24/)
5. [Details and questions](/blog/2015/07/24/)

<!--more-->

## Is event ID unique?

No.

Let’s start with the distribution of the number of events per event ID for a typical Snowplow user.

{% highlight sql %}
SELECT
  count,
  COUNT(*)
FROM (
  SELECT
    event_id,
    COUNT(*) AS count
    FROM atomic.events
  GROUP BY 1
)
GROUP BY 1
ORDER BY 1
{% endhighlight %}

The following SQL query returns, for an example user:

<img src="/assets/img/blog/2015/08/duplicate-events.png" width="368px">

Less than 1% of event IDs have more than one event associated with them.

There are cases where thousands of events have a single event ID. There are even fewer cases where more than 2 events are associated with a single event ID, but it’s a typical long tail distribution. When Snowplow capture billions of events, it’s likely that a few event IDs will have several thousand (or more) events associated with them.

Does this cause issues? In most cases this doesn’t affect the actual use of the data – it’s sufficient to be aware of the existence of duplicates. One situation in which duplicates can cause problem is this:

- multiple contexts are captured with each event
- events are duplicated

Even without duplicates, there might be 5 contexts with the same event ID

## What can cause duplicates?

### ID collision



It’s unlikely that duplicate events are caused by ID collision, even under huge event volumes.

The event ID is generated client-side in the tracker so we can detect both exogenous and endogenous duplicates (which are discussed below).

For example: the Javascript tracker uses UUID V4.

There are some interesting pros and cons to type 1 versus type 4 UUIDs, but as you say, the uniqueness of the UUIDs themselves is already acceptable - this isn't where duplicates come from.


### Exogenous or synthetic duplicates

The second class are exogenous duplicates. These are introduced external to Snowplow by a whole range of systems. This includes: browser pre-cachers, anti-virus software, adult content screeners and web scrapers. These duplicates can be fired before or after the *real* event. They can come from the device itself or from a different IP address. Some of these tools (some crawlers for instance) have limited random number generator functionality, these will then generate the same UUID event after event.

These duplicates share an event ID but only a partial match on other fields (most or all client-sent fields).

either a) delete synthetic copy or b) give it a new ID & preserve relationship to "parent" event

These copies have the same event ID, but parts of the rest of the event can be different.

### Endogenous or natural duplicates

The last class are natural duplicates. These are introduced within the Snowplow pipeline wherever our processing capabilities are set to process events *at least* once:

- The CloudFront collector in the batch flow can duplicate events
- Applications in the Kinesis real-time flow can introduce duplicates because of the KCL checkpointing approach

These events should be deduplicated when events are consumed, and deduplicating consists of deleting all but one event.

## Deduplicating events



Thinking about this further, a simple de-duplication algorithm would be:

If the payload matches exactly, then delete all but one copy
If the payload differs in any way, then there are different options:

- remove these events
- then give one event a new ID and preserve its relationship to "parent" event

Dealing with natural/endogenous duplicates is not hugely difficult - a simple lookup of previously-seen event IDs will suffice. Dealing with synthetic/exogenous duplicates is much more complex - the best solution currently is, as Christophe and Grzegorz say, to use appropriate queries or de-dupe using SQL.

## Deduplicating events in Redshift

Last month, we released [Snowplow R69][r69] (also known as [Blue-Bellied Roller][r69]) which included a new data model which deduplicates events in Redshift.

This addresses an issue where a small percentage of rows have the same event ID. This new model deduplicates natural copies and moves synthetic copies from atomic.events to atomic.duplicated_events. This ensures that the event ID in `atomic.events` is unique.

{% highlight sql %}
SELECT
  event_id
FROM (SELECT event_id, COUNT() AS count FROM atomic.events GROUP BY 1)
WHERE count > 1
{% endhighlight %}

{% highlight sql %}
SELECT * FROM atomic.events
WHERE event_id IN (SELECT event_id FROM atomic.tmp_ids_1)
GROUP BY 1, 2 ... 125
{% endhighlight %}

The `GROUP BY` can be modified to include all relevant columns. -- no, this doesn't work (needs window function as well)

{% highlight sql %}
SELECT
  event_id
FROM (SELECT event_id, COUNT() AS count FROM atomic.tmp_events GROUP BY 1)
WHERE count = 1
{% endhighlight %}

## Roadmap

Note that the ElasticSearch sink for the Kinesis flow has a "last event wins" approach to duplicates: each event is upserted into the ES collection using the event_id, so later dupes will overwrite earlier.

Amazon Kinesis Client Library is build wit assumption that every process has to be processed at leas once. This was the main idea behind check pointing mechanism. The mechanism guarantees, that no data would be missed, but do not ensure single record processing. We should threat this rather like a feature than a bug and not related with Snowplow but Kinesis.

If we have the enriched event stream partitioned by event ID, then we can build a minimal-state* de-duplication engine as a library** for embedding in KCL apps.

* Not stateless. It will need to store event IDs and event fingerprints in DynamoDB to de-dupe across micro-batches.

** De-duplication as a KCL app doesn't work for natural duplicates, because the KCL app can itself introduce natural duplicates. You literally have to embed this library into whatever functionality cares about there being no duplicates.


https://en.wikipedia.org/wiki/Universally_unique_identifier#Version_4_.28random.29
https://en.wikipedia.org/wiki/Universally_unique_identifier#Random%5FUUID%5Fprobability%5Fof%5Fduplicates

[r69]: http://snowplowanalytics.com/blog/2015/07/24/snowplow-r69-blue-bellied-roller-released/
[deduplicate]: https://github.com/snowplow/snowplow/tree/master/5-data-modeling/sql-runner/redshift/sql/deduplicate

https://github.com/snowplow/snowplow/issues/24
https://github.com/snowplow/snowplow/issues/1924
