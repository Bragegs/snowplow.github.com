---
layout: post
shortenedlink: Duplicate event IDs
title: Dealing with duplicate event IDs
tags: [analytics, data modeling, events]
author: Christophe
category: analytics
---

The Snowplow pipeline outputs a data stream in which each line represents a single event. Each event comes with an identifier, the event ID, which was generated by the tracker and is—or rather should be—unique. However, after having used Snowplow for a while, users often notice that some events share an ID. Events are sometimes duplicated within the Snowplow pipeline itself, but it’s often the client-side environment that causes events to be sent in with the same ID.

This blogposts covers:

1. [Is the event ID guaranteed to be unique?](/blog/2015/07/24/)
2. [What are the possible causes?](/blog/2015/07/24/)
4. [Deduplicating events](/blog/2015/07/24/)
5. [Deduplicating events in Redshift](/blog/2015/07/24/)
6. [Deduplicating events in Kinesis (under development)](/blog/2015/07/24/)

<!--more-->

## Is the event ID guaranteed to be unique?

Unfortunately not. Most Snowplow users will find that some events share an event ID. To test this, run the following SQL query in Redshift, which counts the number of events per event ID and returns the overall distribution:

{% highlight sql %}
SELECT
  event_count,
  COUNT(*)
FROM (
  SELECT
    event_id,
    COUNT(*) AS event_count
    FROM atomic.events
  GROUP BY 1
)
GROUP BY 1
ORDER BY 1
{% endhighlight %}

For a typical Snowplow user, and without an additional step that deduplicates the event ID, we expect the distribution to look something like this:

<img src="/assets/img/blog/2015/08/duplicate-events.png" width="368px">

Less than 1% of event IDs have more than one event associated with them.

There are cases where thousands of events have a single event ID. There are even fewer cases where more than 2 events are associated with a single event ID, but it’s a typical long tail distribution. When Snowplow capture billions of events, it’s likely that a few event IDs will have several thousand (or more) events associated with them.

Does this cause

Unstructured events and context are loaded into separate tables in Redshift. These child tables can be joined together or back onto `atomic.events` using `root_id = event_id`. Root ID is the event ID of the parent event.

Does this cause issues? In most cases this doesn’t affect the actual use of the data – it’s sufficient to be aware of the existence of duplicates. One situation in which duplicates can cause problem is this:

- multiple contexts are captured with each event
- events are duplicated

Even without duplicates, there might be 5 contexts with the same event ID

## What can cause duplicates?

### Endogenous or natural duplicates

The last class are natural duplicates. These are introduced within the Snowplow pipeline wherever our processing capabilities are set to process events *at least* once:

- The CloudFront collector in the batch flow can duplicate events
- Applications in the Kinesis real-time flow can introduce duplicates because of the KCL checkpointing approach

These events should be deduplicated when events are consumed, and deduplicating consists of deleting all but one event.

### Exogenous or synthetic duplicates

Note that this algorithm looks at the client-sent fields (whose values are set client-side).

The second class are exogenous duplicates. These are introduced external to Snowplow by a whole range of systems. This includes: browser pre-cachers, anti-virus software, adult content screeners and web scrapers. These duplicates can be fired before or after the *real* event. They can come from the device itself or from a different IP address. Some of these tools (some crawlers for instance) have limited random number generator functionality, these will then generate the same UUID event after event.

These duplicates share an event ID but only a partial match on other fields (most or all client-sent fields).

either a) delete synthetic copy or b) give it a new ID & preserve relationship to "parent" event

These copies have the same event ID, but parts of the rest of the event can be different.

### ID collision

The event ID is generated client-side. The event ID is generated client-side in the tracker so we can detect both exogenous and endogenous duplicates (which are discussed below). It’s, however, unlikely that duplicates are introduced because of ID collision. Even users with large event volumes (billions per day) are safe.

For example: the Javascript tracker uses UUID V4.

There are some interesting pros and cons to type 1 versus type 4 UUIDs, but as you say, the uniqueness of the UUIDs themselves is already acceptable - this isn't where duplicates come from.

## Deduplicating the event ID

We use a simple algorithm to deduplicate the event ID. When 2 or more events share an ID:

- If all client-sent fields match: delete all but one event
- If one or more client-sent fields differ:
  - either assign a new event ID to all but one event and preserve their relationship to the parent event
  - or delete all events

## Deduplicating the event ID in Redshift

Last month, we released [Snowplow 69 Blue-Bellied Roller][r69] with a [new data model][deduplicate] that deduplicates the event ID in Redshift. It consists of a set of SQL queries that can be run on a regular basis (for example, after each load) using our [SQL Runner][sql-runner] application. The queries:

- Deduplicate natural copies and keep them in `atomic.events`
- Remove other duplicates from `atomic.events` and insert them to `atomic.duplicated_events`

This ensures that the event ID in `atomic.events` is unique. The queries can be modified to use different criteria for deduplication and extended to also deduplicate unstructured events and contexts.

Let’s run through the queries. First, we list the event IDs that occur more than once in `atomic.events`:

{% highlight sql %}
CREATE TABLE atomic.tmp_ids_1
  DISTKEY (event_id)
  SORTKEY (event_id)
AS (SELECT event_id FROM (SELECT event_id, COUNT(*) AS count FROM atomic.events GROUP BY 1) WHERE count > 1);
{% endhighlight %}

We use this list to create a table with all events that don’t have a unique event ID:

{% highlight sql %}
CREATE TABLE atomic.tmp_events
  DISTKEY (event_id)
  SORTKEY (event_id)
AS (

  SELECT * FROM atomic.events
  WHERE event_id IN (SELECT event_id FROM atomic.tmp_ids_1)
     OR event_id IN (SELECT event_id FROM atomic.duplicated_events)
  GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9,
  10, 11, 12, 13, 14, 15, 16, 17, 18, 19,
  20, 21, 22, 23, 24, 25, 26, 27, 28, 29,
  30, 31, 32, 33, 34, 35, 36, 37, 38, 39,
  40, 41, 42, 43, 44, 45, 46, 47, 48, 49,
  50, 51, 52, 53, 54, 55, 56, 57, 58, 59,
  60, 61, 62, 63, 64, 65, 66, 67, 68, 69,
  70, 71, 72, 73, 74, 75, 76, 77, 78, 79,
  80, 81, 82, 83, 84, 85, 86, 87, 88, 89,
  90, 91, 92, 93, 94, 95, 96, 97, 98, 99,
  100, 101, 102, 103, 104, 105, 106, 107, 108, 109,
  110, 111, 112, 113, 114, 115, 116, 117, 118, 119,
  120, 121, 122, 123, 124, 125

);
{% endhighlight %}

The `GROUP BY` clause combines natural duplicates into a single row if all columns are equal. It’s also possible to combine duplicates when one or more columns are different (in particular columns that don’t contain client data, such as `etl_tstamp`), but that requires a [window function][redshift-window]. Note that this step might take a while when the absolute number of duplicates is large.

Next, we list the event IDs that have now become unique:

{% highlight sql %}
CREATE TABLE atomic.tmp_ids_2
  DISTKEY (event_id)
  SORTKEY (event_id)
AS (SELECT event_id FROM (SELECT event_id, COUNT(*) AS count FROM atomic.tmp_events GROUP BY 1) WHERE count = 1);
{% endhighlight %}

The last step is wrapped in a [transaction][redshift-begin] to ensure that either all or none of the queries get executed.

First, it deletes the original duplicates from `atomic.events`. Then it inserts the deduplicated natural copies back into the main table, provided that the event ID is not also in `atomic.duplicated_events`. The remaining events are moved to `atomic.duplicated_events`.

{% highlight sql %}
BEGIN;

DELETE FROM atomic.events WHERE event_id IN (SELECT event_id FROM atomic.tmp_ids_1);

INSERT INTO atomic.events (
  SELECT * FROM atomic.tmp_events
  WHERE event_id IN (SELECT event_id FROM atomic.tmp_ids_2)
    AND event_id NOT IN (SELECT event_id FROM atomic.duplicated_events)
);

INSERT INTO atomic.duplicated_events (
  SELECT * FROM atomic.tmp_events
  WHERE event_id NOT IN (SELECT event_id FROM atomic.tmp_ids_2)
    OR event_id IN (SELECT event_id FROM atomic.duplicated_events)
);

COMMIT;
{% endhighlight %}

## Deduplicating the event ID in Kinesis (under development)

The [next step][github-1924] is to bring the deduplication algorithm to Kinesis. We plan to [partition the enriched event stream on event ID][github-1924], then build a minimal-state deduplication engine as a library that can be embedded in [KCL][kcl] apps. The engine will not be stateless because it needs to store event IDs and fingerprints in DynamoDB to deduplicate across micro-batches.

The [Amazon Kinesis Client Library][kcl] is built with the assumption that all processes have to be processed at least once, which was the main idea behind check pointing mechanism. This guarantees that no data is missed, but doesn’t ensure single record processing. Deduplication as a KCL app won’t work for natural duplicates, because the app itself can introduce natural duplicates. You will therefore have to embed the deduplication library in each app that cares about there being no duplicates.

Note that the ElasticSearch sink for the Kinesis flow takes a “last event wins” approach to duplicates. Each event is upserted into the ElasticSearch collection using the event ID, later duplicates will thus overwrite earlier ones.

[uuid-v4]: https://en.wikipedia.org/wiki/Universally_unique_identifier#Version_4_.28random.29
[uuid-random]: https://en.wikipedia.org/wiki/Universally_unique_identifier#Random%5FUUID%5Fprobability%5Fof%5Fduplicates

[r69]: /blog/2015/07/24/snowplow-r69-blue-bellied-roller-released/
[deduplicate]: https://github.com/snowplow/snowplow/tree/master/5-data-modeling/sql-runner/redshift/sql/deduplicate
[sql-runner]: https://github.com/snowplow/sql-runner
[redshift-window]: http://docs.aws.amazon.com/redshift/latest/dg/c_Window_functions.html
[redshift-begin]: http://docs.aws.amazon.com/redshift/latest/dg/r_BEGIN.html

[kcl]: http://docs.aws.amazon.com/kinesis/latest/dev/developing-consumers-with-kcl.html
[github-24]: https://github.com/snowplow/snowplow/issues/24
[github-1924]: https://github.com/snowplow/snowplow/issues/1924
